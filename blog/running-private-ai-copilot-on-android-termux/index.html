<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Running a Private AI Copilot on Android</title>

  <meta name="description" content="How I turned a mid-range Android phone into a completely offline, privacy-first coding assistant using Termux and Qwen 2.5." />

  <meta name="keywords" content="termux, local ai, android development, qwen, llm, privacy, offline coding, productivity" />

  <meta property="og:type" content="article" />
  <meta property="og:title" content="Running a Private AI Copilot on the Redmi Note 10 Pro" />
  <meta property="og:description" content="Turn your Android phone into an offline AI coding assistant." />
  <meta property="og:image" content="../img/local-ai-android.jpg" /> 
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Running a Private AI Copilot on the Redmi Note 10 Pro" />
  <meta name="twitter:description" content="A guide to running local LLMs on mid-range hardware." />
  <meta name="author" content="Abbas Uddin" />

  <meta property="article:tag" content="Termux" />
  <meta property="article:tag" content="Local AI" />
  <meta property="article:tag" content="Android" />
  <meta property="article:tag" content="Productivity" />

  <script src="https://cdn.tailwindcss.com"></script>
  
  <script src="https://unpkg.com/lucide@latest"></script>
  <style>
    @import url("https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@300..700&display=swap");

    body {
      font-family: "Space Grotesk", sans-serif;
    }
    p{
      text-align: justify;
    }
    .prose code::before, .prose code::after {
      content: none; /* Remove backticks added by prose plugin */
    }
    .prose pre {
        background-color: #2A2F35; /* Darker background for code blocks */
        color: #e0e0e0;
        padding: 1em;
        border-radius: 0.5em;
        overflow-x: auto;
    }
    .prose pre code {
        background-color: transparent;
        padding: 0;
        color: inherit;
    }
    /* Highlight command line arguments */
    .cmd-arg {
        color: #00FFC2;
    }
  </style>
</head>

<body class="bg-[#1A1D21] text-white min-h-screen p-6 md:p-12">
  <div class="max-w-4xl mx-auto">
    <a href="../index.html" class="inline-flex items-center mb-8 text-[#00FFC2] hover:underline group transition-all duration-300 ease-in-out">
      <i data-lucide="chevron-left" class="mr-2 h-4 w-4 transition-transform group-hover:-translate-x-1"></i>
      Back to blogs
    </a>

    <article class="bg-[#22262A] rounded-lg overflow-hidden">
      <img src="../img/qwen.webp" alt="Running AI on Android" class="w-full h-64 object-cover" style="object-position: center;" />

      <div class="p-4">
        <h1 class="text-[#00FFC2] text-2xl md:text-4xl font-bold mb-4">Running a Private AI Copilot on Android</h1>

        <div class="flex flex-wrap items-center justify-between mb-8 text-sm text-gray-400">
          <div class="flex items-center space-x-3 mb-3 md:mb-0  px-3 py-2 text-sm border border-[#2A2F35] rounded-full">
            <img src="../../img/abbasuddin.webp" alt="Author Avatar" width="40" height="40" class=" bg-[#2A2F35] rounded-full" />
            <span class="px-3 py-2 text-sm bg-[#2A2F35] rounded-full">Abbas Uddin</span>
          </div>
          <div class="flex items-center space-x-2">
            <span class="flex items-center px-2 py-1 text-sm bg-[#2A2F35] rounded-full">
              <i data-lucide="calendar" class="mr-2 h-4 w-4"></i>
              01/02/2026 </span>
            <span class="flex items-center  px-2 py-1 text-sm bg-[#2A2F35] rounded-full">
              <i data-lucide="clock" class="mr-2 h-4 w-4"></i>
              8 min read </span>
          </div>
        </div>

        <div class="prose prose-invert prose-cyan max-w-none text-gray-300">
          <p>
            As a developer who values privacy and efficiency, I wanted a solution that didn't rely on cloud APIs or monthly subscriptions. I wanted a "Second Brain" that lived in my pocket—specifically, on my <strong>Redmi Note 10 Pro Max</strong>.
          </p>
          <p>
            The challenge? This phone runs on a <strong>Snapdragon 732G</strong> with limited RAM. It's not a flagship. Yet, with the right optimization, I managed to run a <strong>1.5 Billion Parameter</strong> model that helps me write React and Tailwind code completely offline.
          </p>

          <h2 class="font-bold text-[#00FFC2] text-xl mt-8 mb-4">The Hardware Constraints</h2>
          <p>Running AI locally is a game of resource management. Here is the reality of the hardware I was working with:</p>
          <ul class="list-disc pl-5 space-y-2 mb-6">
            <li><strong>Processor:</strong> Snapdragon 732G (Octa-core)</li>
            <li><strong>RAM:</strong> 12GB (8GB Physical + 4GB Swap/Flex)</li>
            <li><strong>Bottleneck:</strong> CPU speed and thermal throttling.</li>
          </ul>

          <h2 class="font-bold text-[#00FFC2] text-xl mt-8 mb-4">The Software Stack</h2>
          <p>To bypass Android's limitations, I used a "Split Stack" approach:</p>
          <ol class="list-decimal pl-5 space-y-2 mb-6">
            <li><strong>The Engine:</strong> <span class="text-[#00FFC2]">Termux</span> running a compiled version of <code>llama.cpp</code>.</li>
            <li><strong>The Model:</strong> <span class="text-[#00FFC2]">Qwen 2.5 (1.5B Instruct)</span> quantized to q4_k_m for memory efficiency.</li>
            <li><strong>The Interface:</strong> <span class="text-[#00FFC2]">ChatterUI</span>, connected via localhost API.</li>
          </ol>

          <h2 class="font-bold text-[#00FFC2] text-xl mt-8 mb-4">Step 1: Compiling the Engine</h2>
          <p>The standard packages weren't enough. I had to build <code>llama.cpp</code> from source to optimize it for my specific hardware.</p>
          <pre><code class="language-bash">pkg update && pkg upgrade
pkg install git cmake clang wget ninja
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

# CPU Build (Most Stable for 732G)
cmake -B build -DGGML_VULKAN=OFF
cmake --build build --config Release -j4</code></pre>

          <h2 class="font-bold text-[#00FFC2] text-xl mt-8 mb-4">Step 2: The "Smart" Shortcut</h2>
          <p>Typing server commands every time is tedious. I created a permanent alias in my <code>.bashrc</code> file. This single command, <code>ai</code>, launches the server with optimal thread settings (4 threads) to prevent overheating.</p>
          <pre><code class="language-bash">alias ai="cd ~/llama.cpp && ./build/bin/llama-server \
-m qwen2.5-1.5b-instruct-q4_k_m.gguf \
-c 2048 \
--host 0.0.0.0 \
--port 8080 \
<span class="cmd-arg">-t 4 -b 512</span>"</code></pre>

          <h2 class="font-bold text-[#00FFC2] text-xl mt-8 mb-4">Step 3: Connecting the Interface</h2>
          <p>I used <strong>ChatterUI</strong> as the frontend. The secret was to configure it as a "Generic OpenAI" connection pointing to the local Termux server:</p>
          
          <ul class="list-disc pl-5 space-y-2 mb-6">
            <li><strong>Base URL:</strong> <code>http://127.0.0.1:8080/v1</code></li>
            <li><strong>API Key:</strong> (Any random string)</li>
            <li><strong>System Prompt:</strong> Custom "Abbas Assistant" persona.</li>
          </ul>

          <h2 class="font-bold text-[#00FFC2] text-xl mt-8 mb-4">The Result</h2>
          <p>I now have a completely offline AI that:</p>
          <ul class="list-disc pl-5 space-y-2 mb-6">
            <li>Knows my coding style (React + Tailwind).</li>
            <li>Costs £0.00 to run.</li>
            <li>Works on the London Underground without signal.</li>
            <li>Keeps my data 100% private.</li>
          </ul>

          <p>
            It’s not ChatGPT-4, but for instant logic checks, regex generation, and boilerplate code, it is faster than unlocking my phone and opening a browser.
          </p>
        </div>

        <div class="mt-12 flex flex-wrap gap-2">
          <span class="px-3 py-1 text-sm bg-[#2A2F35] text-[#00FFC2] rounded-full">Termux</span>
          <span class="px-3 py-1 text-sm bg-[#2A2F35] text-[#00FFC2] rounded-full">Local AI</span>
          <span class="px-3 py-1 text-sm bg-[#2A2F35] text-[#00FFC2] rounded-full">Android</span>
          <span class="px-3 py-1 text-sm bg-[#2A2F35] text-[#00FFC2] rounded-full">Productivity</span>
        </div>

        <div class="mt-8 pt-8 border-t border-gray-700 flex justify-between items-center">
          <button class="flex items-center text-[#00FFC2] hover:underline">
            <i data-lucide="thumbs-up" class="mr-2 h-4 w-4"></i>
            Like this article
          </button>
          <button class="flex items-center text-[#00FFC2] hover:underline">
            <i data-lucide="share-2" class="mr-2 h-4 w-4"></i>
            Share
          </button>
        </div>
      </div>
    </article>
  </div>

  <script>
    // Initialize Lucide Icons
    document.addEventListener("DOMContentLoaded", () => lucide.createIcons());
  </script>
</body>

</html>